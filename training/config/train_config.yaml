mode: train
lmdb: False
dry_run: false

# Point to the parent folder, not directly to UADFV
rgb_dir: './datasets'

# Not needed since lmdb is skipped
lmdb_dir: ''
dataset_json_folder: "C:/ml df project/DeepfakeBench/preprocessing/dataset_json"

SWA: False
save_avg: True
log_dir: ./logs/training/

# label settings
label_dict:
  DFD_fake: 1
  DFD_real: 0
  FF-SH: 1
  FF-F2F: 1
  FF-DF: 1
  FF-FS: 1
  FF-NT: 1
  FF-FH: 1
  FF-real: 0
  CelebDFv1_real: 0
  CelebDFv1_fake: 1
  CelebDFv2_real: 0
  CelebDFv2_fake: 1
  DFDCP_Real: 0
  DFDCP_FakeA: 1
  DFDCP_FakeB: 1
  DFDC_Fake: 1
  DFDC_Real: 0
  DF_fake: 1
  DF_real: 0
  UADFV_Fake: 1
  UADFV_Real: 0
  WebcamDF_Fake: 1
  WebcamDF_Real: 0
  roop_Real: 0
  roop_Fake: 1

# dataset and training settings
train_batchSize: 4     # reduced from 24
test_batchSize: 4      # reduced from 32
workers: 2             # reduced from 8
frame_num: {'train': 4, 'test': 8}   # reduced frames per video
resolution: 224        # reduced from 256
with_mask: false
with_landmark: true
save_ckpt: false
save_feat: false

# data augmentation
use_data_augmentation: false
data_aug:
  flip_prob: 0.5
  rotate_prob: 0.5
  rotate_limit: [-10, 10]
  blur_prob: 0.5
  blur_limit: [3, 7]
  brightness_prob: 0.5
  brightness_limit: [-0.1, 0.1]
  contrast_limit: [-0.1, 0.1]
  quality_lower: 40
  quality_upper: 100

# mean and std for normalization
mean: [0.485, 0.456, 0.406]
std: [0.229, 0.224, 0.225]

# optimizer config
optimizer:
  type: adam
  adam:
    lr: 0.0002
    beta1: 0.9
    beta2: 0.999
    eps: 0.00000001
    weight_decay: 0.0005
    amsgrad: false
  sgd:
    lr: 0.0002
    momentum: 0.9
    weight_decay: 0.0005
  sam:
    lr: 0.001
    momentum: 0.9

# training config
lr_scheduler: step
lr_step: 3
lr_gamma: 0.4
nEpochs: 10          # lowered from 50 for testing
start_epoch: 0
save_epoch: 1
rec_iter: 100
logdir: ./logs
manualSeed: 1024
save_ckpt: false

# loss function
loss_func: cross_entropy
losstype: null

# metric
metric_scoring: auc

# cuda
cuda: true
cudnn: true
